# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_learners.ipynb.

# %% auto 0
__all__ = ['CancelFitException', 'CancelBatchException', 'CancelEpochException', 'Callback', 'run_cbs', 'SingleBatchCB', 'to_cpu',
           'MetricsCB', 'DeviceCB', 'TrainerCB', 'ProgressCB', 'with_cbs', 'Learner', 'TrainLearner', 'MomentumLearner',
           'LRFinderCB', 'lr_find', 'lr_find_test']

# %% ../nbs/09_learners.ipynb 1
import math, torch, matplotlib.pyplot as plt

import fastcore.all as fc
from collections.abc import Mapping
from operator import attrgetter
from functools import partial
from copy import copy

from torch import optim
import torch.nn.functional as F

from .conv import *

from fastprogress import progress_bar,master_bar

# %% ../nbs/09_learners.ipynb 12
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

# %% ../nbs/09_learners.ipynb 13
class Callback(): order = 0

# %% ../nbs/09_learners.ipynb 16
def run_cbs(cbs, method_nm, learn=None):
    for cb in sorted(cbs, key=attrgetter('order')):
        method = getattr(cb, method_nm, None)
        # print(method)
        if method is not None: method(learn)

# %% ../nbs/09_learners.ipynb 22
class SingleBatchCB(Callback):
    order = 1
    def after_batch(self, learn): 
        print('stop training')
        raise CancelFitException()

# %% ../nbs/09_learners.ipynb 24
from torcheval.metrics import MulticlassAccuracy, Mean, BinaryNormalizedEntropy

# %% ../nbs/09_learners.ipynb 27
def to_cpu(x):
    if isinstance(x, Mapping):
        return {k: to_cpu(v) for k, v in x.items()}
    if isinstance(x, list):
        return [to_cpu(o) for o in x]
    if isinstance(x, tuple):
        return tuple(to_cpu(list(x)))
    res = x.detach().cpu()
    return res.float() if res.dtype==torch.float16 else res

# %% ../nbs/09_learners.ipynb 28
class MetricsCB(Callback):
    def __init__(self, *ms, **metrics):
        for o in ms: metrics[type(o).__name__] = o
        self.metrics = metrics
        self.all_metrics = copy(metrics)
        self.all_metrics['loss'] = self.loss = Mean()
        print(self.metrics)
    
    def _log(self, d): print(d)
    def before_fit(self, learn): learn.metrics = self
    def before_epoch(self, learn):
        [o.reset() for o in self.all_metrics.values()]
    
    def after_epoch(self, learn):
        log = {k: f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
        log['epoch'] = learn.epoch
        log['train'] = 'train' if learn.model.training else 'eval'
        self._log(log)
    
    def after_batch(self, learn):
        x,y,*_ = to_cpu(learn.batch)
        for m in self.metrics.values():
            m.update(to_cpu(learn.preds), y)
        self.loss.update(to_cpu(learn.loss), weight=len(x))

# %% ../nbs/09_learners.ipynb 29
class DeviceCB(Callback):
    def __init__(self, device=def_device): fc.store_attr()

    def before_fit(self, learn):
        if hasattr(learn.model, 'to'): learn.model.to(self.device)
    
    def before_batch(self, learn):
        learn.batch = to_device(learn.batch, device=self.device)

# %% ../nbs/09_learners.ipynb 38
class TrainerCB(Callback):
    def __init__(self, n_inp=1): fc.store_attr()
    def predict(self, learn):
        learn.preds = learn.model(*learn.batch[:self.n_inp])
    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])
    def backward(self, learn): learn.loss.backward()
    def step(self, learn): learn.opt.step()
    def zero_grad(self, learn): learn.opt.zero_grad()

# %% ../nbs/09_learners.ipynb 41
class ProgressCB(Callback):
    order = MetricsCB.order + 1
    def __init__(self, plot=False): fc.store_attr()
    def before_fit(self, learn):
        learn.epochs = self.mbar = master_bar(learn.epochs)
        self.first = True
        if hasattr(learn, 'metrics'): learn.metrics._log = self._log
        self.train_losses, self.val_losses = [], []
    
    def _log(self, d):
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first = False
        self.mbar.write(list(d.values()), table=True)
    
    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)
    def after_batch(self, learn):
        learn.dl.comment = f"{learn.loss:.3f}"
        if self.plot and hasattr(learn, 'metrics') and learn.training:
            self.train_losses.append(learn.loss.item())
            if self.val_losses:
                self.mbar.update_graph([
                    [fc.L.range(self.train_losses), self.train_losses],
                    [fc.L.range(learn.epoch).map(lambda x: (x+1) * len(learn.dls.train)), self.val_losses]
                ])
    
    def after_epoch(self, learn):
        if not learn.training and self.plot and hasattr(learn, 'metrics'):
            self.val_losses.append(learn.metrics.all_metrics['loss'].compute())
            # print(f"len val_losses = {len(self.val_losses)}, epoch={learn.epoch}")
            # learn.epoch has fisished, so learn.epoch + 1 = len(self.vasl_losses) here.
            self.mbar.update_graph([
                    [fc.L.range(self.train_losses), self.train_losses],
                    [fc.L.range(learn.epoch+1).map(lambda x: (x+1) * len(learn.dls.train)), self.val_losses]
                ])

# %% ../nbs/09_learners.ipynb 42
class with_cbs:
    def __init__(self, nm): fc.store_attr()
    def __call__(self, f):
        # print(self.nm)
        def _f(o, *args, **kwargs):
            try:
                o.callback(f"before_{self.nm}")
                f(o, *args, **kwargs)
                o.callback(f"after_{self.nm}")
            except globals()[f'Cancel{self.nm.title()}Exception']: pass
            finally: o.callback(f'cleanup_{self.nm}')
        
        return _f

# %% ../nbs/09_learners.ipynb 46
class Learner:
    def __init__(self, model, dls=(0,), loss_func=F.cross_entropy, cbs=[], lr=0.1, opt_func=optim.SGD):
        self.model = model
        self.dls = dls
        self.loss_func = loss_func
        self.cbs = fc.L(cbs)
        self.lr = lr
        self.opt_func = opt_func
        # fc.store_attr()
    
    @with_cbs('batch')
    def _one_batch(self):
        self.predict()
        self.callback('after_predict')
        self.get_loss()
        self.callback('after_loss')
        if self.training:
            self.backward()
            self.callback('after_backward')
            self.step()
            self.callback('after_step')
            self.zero_grad()
    
    @with_cbs('epoch')
    def _one_epoch(self):
        for self.iter, self.batch in enumerate(self.dl): self._one_batch()
    
    def one_epoch(self, training):
        self.model.train(training)
        self.dl = self.dls.train if training else self.dls.valid
        self._one_epoch()
    
    @with_cbs('fit')
    def _fit(self, train, valid):
        for self.epoch in self.epochs:
            if train: self.one_epoch(True)
            if valid:
                with torch.no_grad(): self.one_epoch(False)
    
    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):
        cbs = fc.L(cbs)
        try:
            for cb in cbs:
                self.cbs.append(cb)
            self.epochs = range(n_epochs)
            if lr is None: lr = self.lr
            self.opt = self.opt_func(self.model.parameters(), lr=lr)
            self._fit(train, valid)
        finally:
            for cb in cbs: self.cbs.remove(cb)
    
    def callback(self, method_name):
        run_cbs(self.cbs, method_name, self)
    
    def __getattr__(self, name):
        if name in ('predict', 'get_loss', 'backward', 'step', 'zero_grad'):
            # print (partial(self.callback, name))
            return partial(self.callback, name)
    
    @property
    def training(self): return self.model.training

# %% ../nbs/09_learners.ipynb 55
class TrainLearner(Learner):
    def predict(self): self.preds = self.model(self.batch[0])
    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])
    def backward(self): self.loss.backward()
    def step(self): self.opt.step()
    def zero_grad(self): self.opt.zero_grad()

# %% ../nbs/09_learners.ipynb 57
class MomentumLearner(TrainLearner):
    def __init__(self, model, dls=(0,), loss_func=F.cross_entropy, cbs=[], lr=0.1, opt_func=optim.SGD, mom=0.8):
        self.mom = mom
        super().__init__(model, dls, loss_func, cbs, lr, opt_func)
    
    def zero_grad(self):
        with torch.no_grad():
            for p in self.model.parameters():
                # print(p.grad)
                # if p.requires_grad:
                p.grad *= self.mom

# %% ../nbs/09_learners.ipynb 59
from torch.optim.lr_scheduler import ExponentialLR
import pandas as pd

# %% ../nbs/09_learners.ipynb 60
class LRFinderCB(Callback):
    def __init__(self, gamma=1.3, max_mult=3):
        fc.store_attr()
    
    def before_fit(self, learn):
        self.sched = ExponentialLR(learn.opt, self.gamma)
        self.lrs,self.losses = [],[]
        self.min = math.inf
    
    def after_batch(self, learn):
        if not learn.training:
            raise CancelEpochException()
        # last_lr = learn.opt.param_groups[0]['lr']
        last_lr = self.sched.get_last_lr()[0]
        self.lrs.append(last_lr)
        loss = learn.loss.item()
        self.losses.append(loss)
        if loss < self.min: self.min = loss
        if math.isnan(loss) or (loss > self.min * self.max_mult):
            raise CancelFitException()
        # increase the learning rate by multiplying by gamma
        self.sched.step()
    
    def cleanup_fit(self, learn):
        # print(f"learning rates are {self.lrs}, losses are {self.losses}")

        # lr_losses_pd = pd.DataFrame(
        #     {'lrs': self.lrs,
        #      'losses': self.losses
        #     })
        # print(lr_losses_pd)
        plt.plot(self.lrs, self.losses)
        plt.xscale('log')

# %% ../nbs/09_learners.ipynb 63
@fc.patch
def lr_find(self: Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):
        self.fit(max_epochs, train=True, valid=False, lr=start_lr, cbs=LRFinderCB(max_mult=max_mult, gamma=gamma))

# %% ../nbs/09_learners.ipynb 65
@fc.patch
def lr_find_test(self: Learner, gamma=1.3):
    print(f'gamma is {gamma}')
