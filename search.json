[
  {
    "objectID": "02_meanshift.html",
    "href": "02_meanshift.html",
    "title": "Clustering",
    "section": "",
    "text": "import math, matplotlib.pyplot as plt, operator, torch\n\nfrom functools import partial\n\n\n\nimport torch\nfrom torch import tensor\n\n\n# !jt -r\n\nReset css and font defaults in:\n/Users/htkumar/.jupyter/custom &\n/Users/htkumar/Library/Jupyter/nbextensions\n\n\n\n# !jt -t monokai\n\n\ntorch.manual_seed(42)\ntorch.set_printoptions(precision=3, linewidth=140, sci_mode=False)\n\n\nCreate data\n\nn_clusters=6\nn_samples=250\n\n\ncentroids = torch.rand(n_clusters, 2)*70-35\n\n\ncentroids\n\ntensor([[ 26.759,  29.050],\n        [ -8.200,  32.151],\n        [ -7.669,   7.063],\n        [-17.040,  20.555],\n        [ 30.854, -25.677],\n        [ 30.422,   6.551]])\n\n\n\ncentroids.shape\n\ntorch.Size([6, 2])\n\n\n\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\n\n# MultivariateNormal??\n\n\n# torch.eye??\n\n\ndef sample(m):\n    return MultivariateNormal(m, torch.diag(tensor([5., 5.]))).sample((n_samples, ))\n\n\nslices = [sample(c) for c in centroids]\n\n\ntorch.cat??\n\n\ndata = torch.cat(slices)\ndata.shape\n\ntorch.Size([1500, 2])\n\n\n\ndef plot_data(centroids, data, n_samples, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    \n    for i, centroid in enumerate(centroids):\n        samples = data[i*n_samples: (i+1)*n_samples]\n        ax.scatter(samples[:,0], samples[:,1], s=1)\n        ax.plot(*centroid, markersize=10, marker=\"x\", color=\"k\", mew=5)\n        ax.plot(*centroid, markersize=5, marker=\"x\", color=\"m\", mew=2)\n\n\nplot_data(centroids, data, n_samples)"
  },
  {
    "objectID": "06_foundations.html",
    "href": "06_foundations.html",
    "title": "concepts to know about",
    "section": "",
    "text": "partial\ncall for classes\n*args and **kwargs\ncallbacks\ndunder thingies\n\n\n\nimport torch\nimport matplotlib.pyplot as plt\nimport random\n\n\nCallbacks\n\nimport ipywidgets as widgets\n\n\nw = widgets.Button(description='Click me')\n\n\ndef f(o): print('hi')\n\n\nw.on_click(f)\n\n\nw\n\n\n\n\n\nw.click()\n\nhi\n\n\n\nfrom time import sleep\n\n\ndef slow_calculation():\n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1)\n    return res\n\n\nslow_calculation()\n\n30\n\n\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        res += i*i\n        sleep(1)\n        if cb:\n            cb(i)\n    return res\n\n\ndef show_progress(epoch):\n    print(f'Awesome! Epoch: {epoch}')\n\n\nslow_calculation(show_progress)\n\nAwesome! Epoch: 0\nAwesome! Epoch: 1\nAwesome! Epoch: 2\nAwesome! Epoch: 3\nAwesome! Epoch: 4\n\n\n30\n\n\n\nslow_calculation(lambda epoch: print(f'Awesome! Epoch: {epoch}'))\n\nAwesome! Epoch: 0\nAwesome! Epoch: 1\nAwesome! Epoch: 2\nAwesome! Epoch: 3\nAwesome! Epoch: 4\n\n\n30\n\n\n\ndef show_progress(exclamation, epoch):\n    print(f'Awesome! {exclamation}, epoch: {epoch}')\n\n\nslow_calculation(lambda epoch: show_progress('Job', epoch))\n\nAwesome! Job, epoch: 0\nAwesome! Job, epoch: 1\nAwesome! Job, epoch: 2\nAwesome! Job, epoch: 3\nAwesome! Job, epoch: 4\n\n\n30\n\n\n\ndef f(a, b, c):\n    print(a, b, c)\n\n\nfrom functools import partial\n\n\nf(1, 2, 3)\n\n1 2 3\n\n\n\ng = partial(f, 1, 2)\n\n\ng(3)\n\n1 2 3\n\n\n\ndef make_show_progress(exclamation):\n    def _inner(epoch):\n        print(f'Awesome job {exclamation}, epoch: {epoch}')\n    return _inner\n\n\nslow_calculation(make_show_progress(\"Nice\"))\n\nAwesome job Nice, epoch: 0\nAwesome job Nice, epoch: 1\nAwesome job Nice, epoch: 2\nAwesome job Nice, epoch: 3\nAwesome job Nice, epoch: 4\n\n\n30\n\n\n\nslow_calculation(partial(show_progress, 'Nice'))\n\nAwesome! Nice, epoch: 0\nAwesome! Nice, epoch: 1\nAwesome! Nice, epoch: 2\nAwesome! Nice, epoch: 3\nAwesome! Nice, epoch: 4\n\n\n30\n\n\n\n\nCallbacks as callable classes\n\nclass ProgressShowingCallback():\n    def __init__(self, exclamation): self.exclamation = exclamation\n    def __call__(self, epoch):\n        print(f'Awesome! {self.exclamation}, epoch: {epoch}')\n\n\ncb = ProgressShowingCallback(\"Nice\")\n\n\ncb(1)\n\nAwesome! Nice, epoch: 1\n\n\n\nslow_calculation(cb)\n\nAwesome! Nice, epoch: 0\nAwesome! Nice, epoch: 1\nAwesome! Nice, epoch: 2\nAwesome! Nice, epoch: 3\nAwesome! Nice, epoch: 4\n\n\n30\n\n\n\n\nMultiple callback funcs; *args and **kwargs\n\ndef f(*args, **kwargs):\n    # print(type(args), type(kwargs))\n    print(f\"args: {args}, kwargs: {kwargs}\")\n\n\nf(3, 'a', thing1='hello')\n\nargs: (3, 'a'), kwargs: {'thing1': 'hello'}\n\n\n\ndef g(a, b, c=0):\n    print(a, b, c)\n\n\nargs=[1,2]\nkwargs={'c': 3}\ng(*args, **kwargs)\n\n1 2 3\n\n\n\nPassing args and kwargs\nFunctions which accept these as params\nCan be used to abosrb all the unused args as well\n\n\ndef slow_calculation(cb=None):\n    res = 0\n    for i in range(5):\n        if cb: cb.before_calc(i)\n        res += i*i\n        sleep(1)\n        if cb: cb.after_calc(i, res)\n    \n    return res\n\n\nclass PrintStepCallback():\n    def before_calc(self, *args, **kwargs): print('About to start')\n    def after_calc(self, *args, **kwarfs): print('Done')\n\n\nslow_calculation(PrintStepCallback())\n\nAbout to start\nDone\nAbout to start\nDone\nAbout to start\nDone\nAbout to start\nDone\nAbout to start\nDone\n\n\n30\n\n\n\nclass PrintStatusCallback():\n    def before_calc(self, *args, **kwargs): print('About to start')\n    def after_calc(self, *args, **kwargs): print(f\"Epoch: {args[0]}, val: {args[1]}\")\n\n\nslow_calculation(PrintStatusCallback())\n\nAbout to start\nEpoch: 0, val: 0\nAbout to start\nEpoch: 1, val: 1\nAbout to start\nEpoch: 2, val: 5\nAbout to start\nEpoch: 3, val: 14\nAbout to start\nEpoch: 4, val: 30\n\n\n30\n\n\n\nclass SlowCalculator():\n    def __init__(self, cb=None):\n        self.cb,self.res = cb, 0\n        \n    def callback(self, cb_name, *args):\n        if not self.cb:\n            return\n        cb = getattr(self.cb, cb_name, None)\n        if cb: return cb(self, *args)\n    \n    def calc(self):\n        for i in range(5):\n            self.callback('before_calc', i)\n            self.res += i * i\n            sleep(1)\n            if self.callback('after_calc', i):\n                print('early stop')\n                break\n\n\nclass ModifyingCallback():\n    def after_calc(self, calc, epoch):\n        print(f'After {epoch}: {calc.res}')\n        if calc.res &gt; 10:\n            return True\n        if calc.res &lt; 3:\n            calc.res = calc.res * 2\n\n\nc = SlowCalculator(ModifyingCallback())\n\n\nc.calc()\n\nAfter 0: 0\nAfter 1: 1\nAfter 2: 6\nAfter 3: 15\nearly stop\n\n\n\nc.res\n\n15\n\n\n\ngetattr(c, 'calc')()\n\nAfter 0: 0\nAfter 1: 1\nAfter 2: 6\nAfter 3: 15\nearly stop\n\n\n\nclass SloppyAdder():\n    def __init__(self, o): self.o=o\n    def __add__(self, s): return SloppyAdder(self.o + s.o + 0.01)\n    def __repr__(self): return str(self.o)\n\n\ns1 = SloppyAdder(1)\ns2 = SloppyAdder(2)\ns1+s2\n\n3.01\n\n\n\nclass B:\n    a,b=1,2\n    def __getattr__(self, k):\n        if k[0]=='_': raise AttributeError(k)\n        return f'Hello {k}'\n    \n    def abcd(self):\n        return 'abcd'\n\n\nb = B()\n\n\nb.a\n\n1\n\n\n\nb.foo\n\n'Hello foo'\n\n\n\ngetattr(b, 'a')\n\n1\n\n\n\ngetattr(b, 'abc')\n\n'Hello abc'\n\n\n\nb.abcd()\n\n'abcd'\n\n\n\nhasattr(b, 'abcd')\n\nTrue"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fastai_course",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "fastai_course",
    "section": "Install",
    "text": "Install\npip install fastai_course"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "fastai_course",
    "section": "How to use",
    "text": "How to use\nFill me in please! Donâ€™t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "04_minibatch_training.html",
    "href": "04_minibatch_training.html",
    "title": "fastai_course",
    "section": "",
    "text": "from fastcore.test import test_close"
  },
  {
    "objectID": "01_matmul.html",
    "href": "01_matmul.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "from pathlib import Path\n\n\n\nfrom urllib.request import urlretrieve\n\n\nurlretrieve?\n\nSignature: urlretrieve(url, filename=None, reporthook=None, data=None)\nDocstring:\nRetrieve a URL into a temporary location on disk.\n\nRequires a URL argument. If a filename is passed, it is used as\nthe temporary file location. The reporthook argument should be\na callable that accepts a block number, a read size, and the\ntotal file size of the URL target. The data argument should be\nvalid URL encoded data.\n\nIf a filename is passed and the URL points to a local resource,\nthe result is a copy from local file to new file.\n\nReturns a tuple containing the path to the newly created\ndata file as well as the resulting HTTPMessage object.\nFile:      /opt/anaconda3/lib/python3.9/urllib/request.py\nType:      function\n\n\n\nimport pickle, gzip, math, os, time, shutil, matplotlib as mpl, matplotlib.pyplot as plt\n\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\n\n\npath_data = Path('data')\nprint(path_data)\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\n\ndata\n\n\n\nfrom urllib.request import urlretrieve\nif not path_gz.exists():\n    urlretrieve(MNIST_URL, path_gz)\n\n\n!ls -l data\n\ntotal 33312\n-rw-r--r--  1 htkumar  staff  17051982 Oct 12 00:34 mnist.pkl.gz\n\n\n\nwith gzip.open(path_gz, 'rb') as f:\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n\ntype(x_train), x_train.shape\n\n(numpy.ndarray, (50000, 784))\n\n\n\nlst1 = list(x_train[0])\nvals = lst1[200: 210]\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\nvals\n\n[0.0,\n 0.0,\n 0.0,\n 0.19140625,\n 0.9296875,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125,\n 0.98828125]\n\n\n\ndef chunks(x, sz):\n    for i in range(0, len(x), sz):\n        yield x[i:i+sz]\n\n\nlist(chunks(vals, 5))\n\n[[0.0, 0.0, 0.0, 0.19140625, 0.9296875],\n [0.98828125, 0.98828125, 0.98828125, 0.98828125, 0.98828125]]\n\n\n\nlen(lst1)\n\n784\n\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(lst1, 28)));\n\n\n\n\n\nfrom itertools import islice\n\n\nit = iter(vals)\nislice(it, 5)\n\n&lt;itertools.islice at 0x7fd540d54720&gt;\n\n\n\nlist(islice(it, 5))\n\n[0.0, 0.0, 0.0, 0.19140625, 0.9296875]\n\n\n\nit = iter(lst1)\nimg = list(iter(lambda: list(islice(it, 28)), []))\n\n\nplt.imshow(img);\n\n\n\n\n\niter??\n\nDocstring:\niter(iterable) -&gt; iterator\niter(callable, sentinel) -&gt; iterator\n\nGet an iterator from an object.  In the first form, the argument must\nsupply its own iterator, or be a sequence.\nIn the second form, the callable is called until it returns the sentinel.\nType:      builtin_function_or_method\n\n\n\nlen(img[0]), len(img)\n\n(28, 28)\n\n\n\nimg[20][15]\n\n0.98828125\n\n\n\nclass Matrix:\n    def __init__(self, xs):\n        self.xs = xs\n    def __getitem__(self, i):\n        return self.xs[i[0]][i[1]]\n\n\nm = Matrix(img)\nm[20, 15]\n\n0.98828125\n\n\n\nimport torch\n\n: \n\n\n\nimport torch\nfrom torch import tensor\n\n\ntensor(vals)\n\ntensor([0.0000, 0.0000, 0.0000, 0.1914, 0.9297, 0.9883, 0.9883, 0.9883, 0.9883,\n        0.9883])\n\n\n\nx_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\nx_train.shape\n\ntorch.Size([50000, 784])\n\n\n\nx_train.type()\n\n'torch.FloatTensor'\n\n\n\nimgs = x_train.reshape((-1, 28, 28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nplt.imshow(imgs[0]);\n\n\n\n\n\nimgs[0,20,15]\n\ntensor(0.9883)\n\n\n\nn,c = x_train.shape\nn, c\n\n(50000, 784)\n\n\n\ny_train, y_train.shape\n\n(tensor([5, 0, 4,  ..., 8, 4, 8]), torch.Size([50000]))\n\n\n\nmin(y_train), max(y_train)\n\n(tensor(0), tensor(9))\n\n\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))\n\n\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\n\nseed(457428938475)\nrnd_state\n\n(4976, 20238, 499)\n\n\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\n\nrand(), rand(), rand()\n\n(0.7645251082582081, 0.7920889799553945, 0.06912886811267205)\n\n\n\nplt.plot([rand() for _ in range(50)]);\n\n\n\n\n\nplt.plot??\n\nSignature: plt.plot(*args, scalex=True, scaley=True, data=None, **kwargs)\nDocstring:\nPlot y versus x as lines and/or markers.\n\nCall signatures::\n\n    plot([x], y, [fmt], *, data=None, **kwargs)\n    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n\nThe coordinates of the points or line nodes are given by *x*, *y*.\n\nThe optional parameter *fmt* is a convenient way for defining basic\nformatting like color, marker and linestyle. It's a shortcut string\nnotation described in the *Notes* section below.\n\n&gt;&gt;&gt; plot(x, y)        # plot x and y using default line style and color\n&gt;&gt;&gt; plot(x, y, 'bo')  # plot x and y using blue circle markers\n&gt;&gt;&gt; plot(y)           # plot y using x as index array 0..N-1\n&gt;&gt;&gt; plot(y, 'r+')     # ditto, but with red plusses\n\nYou can use `.Line2D` properties as keyword arguments for more\ncontrol on the appearance. Line properties and *fmt* can be mixed.\nThe following two calls yield identical results:\n\n&gt;&gt;&gt; plot(x, y, 'go--', linewidth=2, markersize=12)\n&gt;&gt;&gt; plot(x, y, color='green', marker='o', linestyle='dashed',\n...      linewidth=2, markersize=12)\n\nWhen conflicting with *fmt*, keyword arguments take precedence.\n\n\n**Plotting labelled data**\n\nThere's a convenient way for plotting objects with labelled data (i.e.\ndata that can be accessed by index ``obj['y']``). Instead of giving\nthe data in *x* and *y*, you can provide the object in the *data*\nparameter and just give the labels for *x* and *y*::\n\n&gt;&gt;&gt; plot('xlabel', 'ylabel', data=obj)\n\nAll indexable objects are supported. This could e.g. be a `dict`, a\n`pandas.DataFrame` or a structured numpy array.\n\n\n**Plotting multiple sets of data**\n\nThere are various ways to plot multiple sets of data.\n\n- The most straight forward way is just to call `plot` multiple times.\n  Example:\n\n  &gt;&gt;&gt; plot(x1, y1, 'bo')\n  &gt;&gt;&gt; plot(x2, y2, 'go')\n\n- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n  for every column. If both *x* and *y* are 2D, they must have the\n  same shape. If only one of them is 2D with shape (N, m) the other\n  must have length N and will be used for every data set m.\n\n  Example:\n\n  &gt;&gt;&gt; x = [1, 2, 3]\n  &gt;&gt;&gt; y = np.array([[1, 2], [3, 4], [5, 6]])\n  &gt;&gt;&gt; plot(x, y)\n\n  is equivalent to:\n\n  &gt;&gt;&gt; for col in range(y.shape[1]):\n  ...     plot(x, y[:, col])\n\n- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n  groups::\n\n  &gt;&gt;&gt; plot(x1, y1, 'g^', x2, y2, 'g-')\n\n  In this case, any additional keyword argument applies to all\n  datasets. Also this syntax cannot be combined with the *data*\n  parameter.\n\nBy default, each line is assigned a different style specified by a\n'style cycle'. The *fmt* and line property parameters are only\nnecessary if you want explicit deviations from these defaults.\nAlternatively, you can also change the style cycle using\n:rc:`axes.prop_cycle`.\n\n\nParameters\n----------\nx, y : array-like or scalar\n    The horizontal / vertical coordinates of the data points.\n    *x* values are optional and default to ``range(len(y))``.\n\n    Commonly, these parameters are 1D arrays.\n\n    They can also be scalars, or two-dimensional (in that case, the\n    columns represent separate data sets).\n\n    These arguments cannot be passed as keywords.\n\nfmt : str, optional\n    A format string, e.g. 'ro' for red circles. See the *Notes*\n    section for a full description of the format strings.\n\n    Format strings are just an abbreviation for quickly setting\n    basic line properties. All of these and more can also be\n    controlled by keyword arguments.\n\n    This argument cannot be passed as keyword.\n\ndata : indexable object, optional\n    An object with labelled data. If given, provide the label names to\n    plot in *x* and *y*.\n\n    .. note::\n        Technically there's a slight ambiguity in calls where the\n        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n        the former interpretation is chosen, but a warning is issued.\n        You may suppress the warning by adding an empty format string\n        ``plot('n', 'o', '', data=obj)``.\n\nReturns\n-------\nlist of `.Line2D`\n    A list of lines representing the plotted data.\n\nOther Parameters\n----------------\nscalex, scaley : bool, default: True\n    These parameters determine if the view limits are adapted to the\n    data limits. The values are passed on to `autoscale_view`.\n\n**kwargs : `.Line2D` properties, optional\n    *kwargs* are used to specify properties like a line label (for\n    auto legends), linewidth, antialiasing, marker face color.\n    Example::\n\n    &gt;&gt;&gt; plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n    &gt;&gt;&gt; plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n\n    If you specify multiple lines with one plot call, the kwargs apply\n    to all those lines. In case the label object is iterable, each\n    element is used as labels for each set of data.\n\n    Here is a list of available `.Line2D` properties:\n\n    Properties:\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n    alpha: scalar or None\n    animated: bool\n    antialiased or aa: bool\n    clip_box: `.Bbox`\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    color or c: color\n    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    dashes: sequence of floats (on/off ink in points) or (None, None)\n    data: (2, N) array or two 1D arrays\n    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n    figure: `.Figure`\n    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n    gid: str\n    in_layout: bool\n    label: object\n    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n    linewidth or lw: float\n    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n    markeredgecolor or mec: color\n    markeredgewidth or mew: float\n    markerfacecolor or mfc: color\n    markerfacecoloralt or mfcalt: color\n    markersize or ms: float\n    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n    path_effects: `.AbstractPathEffect`\n    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n    pickradius: float\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n    transform: unknown\n    url: str\n    visible: bool\n    xdata: 1D array\n    ydata: 1D array\n    zorder: float\n\nSee Also\n--------\nscatter : XY scatter plot with markers of varying size and/or color (\n    sometimes also called bubble chart).\n\nNotes\n-----\n**Format Strings**\n\nA format string consists of a part for color, marker and line::\n\n    fmt = '[marker][line][color]'\n\nEach of them is optional. If not provided, the value from the style\ncycle is used. Exception: If ``line`` is given, but no ``marker``,\nthe data will be a line without markers.\n\nOther combinations such as ``[color][marker][line]`` are also\nsupported, but note that their parsing may be ambiguous.\n\n**Markers**\n\n=============   ===============================\ncharacter       description\n=============   ===============================\n``'.'``         point marker\n``','``         pixel marker\n``'o'``         circle marker\n``'v'``         triangle_down marker\n``'^'``         triangle_up marker\n``'&lt;'``         triangle_left marker\n``'&gt;'``         triangle_right marker\n``'1'``         tri_down marker\n``'2'``         tri_up marker\n``'3'``         tri_left marker\n``'4'``         tri_right marker\n``'8'``         octagon marker\n``'s'``         square marker\n``'p'``         pentagon marker\n``'P'``         plus (filled) marker\n``'*'``         star marker\n``'h'``         hexagon1 marker\n``'H'``         hexagon2 marker\n``'+'``         plus marker\n``'x'``         x marker\n``'X'``         x (filled) marker\n``'D'``         diamond marker\n``'d'``         thin_diamond marker\n``'|'``         vline marker\n``'_'``         hline marker\n=============   ===============================\n\n**Line Styles**\n\n=============    ===============================\ncharacter        description\n=============    ===============================\n``'-'``          solid line style\n``'--'``         dashed line style\n``'-.'``         dash-dot line style\n``':'``          dotted line style\n=============    ===============================\n\nExample format strings::\n\n    'b'    # blue markers with default shape\n    'or'   # red circles\n    '-g'   # green solid line\n    '--'   # dashed line with default color\n    '^k:'  # black triangle_up markers connected by a dotted line\n\n**Colors**\n\nThe supported color abbreviations are the single letter codes\n\n=============    ===============================\ncharacter        color\n=============    ===============================\n``'b'``          blue\n``'g'``          green\n``'r'``          red\n``'c'``          cyan\n``'m'``          magenta\n``'y'``          yellow\n``'k'``          black\n``'w'``          white\n=============    ===============================\n\nand the ``'CN'`` colors that index into the default property cycle.\n\nIf the color is the only part of the format string, you can\nadditionally use any  `matplotlib.colors` spec, e.g. full names\n(``'green'``) or hex strings (``'#008000'``).\nSource:   \n@_copy_docstring_and_deprecators(Axes.plot)\ndef plot(*args, scalex=True, scaley=True, data=None, **kwargs):\n    return gca().plot(\n        *args, scalex=scalex, scaley=scaley,\n        **({\"data\": data} if data is not None else {}), **kwargs)\nFile:      /opt/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\nplt.hist([rand() for _ in range(10_000)])\n\n(array([ 959.,  972., 1036., 1019., 1037.,  971.,  997., 1000., 1036.,\n         973.]),\n array([6.60514437e-06, 1.00005135e-01, 2.00003666e-01, 3.00002196e-01,\n        4.00000726e-01, 4.99999256e-01, 5.99997787e-01, 6.99996317e-01,\n        7.99994847e-01, 8.99993378e-01, 9.99991908e-01]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\nThe slowest run took 4.24 times longer than the fastest. This could mean that an intermediate result is being cached.\n127 Âµs Â± 95.4 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n\n\n\ntorch.manual_seed(1)\nweights = torch.randn(784, 10)\nbias = torch.zeros(10)\n\n\nm1 = x_valid[:5]\nm2 = weights\n\n\nm1.shape, m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nar,ac = m1.shape\nbr,bc = m2.shape\n(ar,ac), (br, bc)\n\n((5, 784), (784, 10))\n\n\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\nfor i in range(ar):\n    for j in range(bc):\n        for k in range(ac):\n            t1[i,j] += m1[i,k] * m2[k,j]\n\n\nt1\n\ntensor([[-10.9417,  -0.6844,  -7.0038,  -4.0066,  -2.0857,  -3.3588,   3.9127,\n          -3.4375, -11.4696,  -2.1153],\n        [ 14.5430,   5.9977,   2.8914,  -4.0777,   6.5914, -14.7383,  -9.2787,\n           2.1577, -15.2772,  -2.6758],\n        [  2.2204,  -3.2171,  -4.7988,  -6.0453,  14.1661,  -8.9824,  -4.7922,\n          -5.4446, -20.6758,  13.5657],\n        [ -6.7097,   8.8998,  -7.4611,  -7.8966,   2.6994,  -4.7260, -11.0278,\n         -12.9776,  -6.4443,   3.6376],\n        [ -2.4444,  -6.4034,  -2.3984,  -9.0371,  11.1772,  -5.7724,  -8.9214,\n          -3.7862,  -8.9827,   5.2797]])\n\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n\n\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ndef matmul(a, b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac):\n                c[i,j] += m1[i,k] * m2[k,j]\n    return c\n\n\n\n\nCPU times: user 580 ms, sys: 4.72 ms, total: 585 ms\nWall time: 588 ms\n\n\n\nNumba\n\nfrom numba import njit\n\n\n@njit\ndef dot(a, b):\n    res = 0.\n    for i in range(len(a)): res += a[i]*b[i]\n    return res\n\n\nfrom numpy import array\n\n\n\n\nCPU times: user 215 ms, sys: 34.2 ms, total: 250 ms\nWall time: 338 ms\n\n\n20.0\n\n\n\n\n\nCPU times: user 25 Âµs, sys: 0 ns, total: 25 Âµs\nWall time: 26.7 Âµs\n\n\n20.0\n\n\n\ndef matmul(a, b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc): c[i,j] = dot(a[i,:], b[:,j])\n    return c\n\n\nm1a,m2a = m1.numpy(),m2.numpy()\n\n\nfrom fastcore.test import *\n\n\ntest_close(t1, matmul(m1a,m2a))\n\n\n\n\n351 Âµs Â± 22.2 Âµs per loop (mean Â± std. dev. of 7 runs, 50 loops each)\n\n\n\n\n\nCPU times: user 1.55 ms, sys: 1.12 ms, total: 2.67 ms\nWall time: 2.01 ms\n\n\n\n\nElementwise ops\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\n\na+b\n\ntensor([12., 14.,  3.])\n\n\n\n(a&lt;b).float().mean()\n\ntensor(0.67)\n\n\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nsf = (m*m).sum()\nsf\n\ntensor(285.)\n\n\n\nsf.sqrt()\n\ntensor(16.88)\n\n\n\nm[2,:], m[:,2]\n\n(tensor([7., 8., 9.]), tensor([3., 6., 9.]))\n\n\n\ndef matmul(a, b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            c[i,j] = (a[i] * b[:,j]).sum()\n    return c\n\n\ntest_close(t1, matmul(m1, m2))\n\n\ntest_close??\n\nSignature: test_close(a, b, eps=1e-05)\nSource:   \ndef test_close(a,b,eps=1e-5):\n    \"`test` that `a` is within `eps` of `b`\"\n    test(a,b,partial(is_close,eps=eps),'close')\nFile:      /opt/anaconda3/lib/python3.9/site-packages/fastcore/test.py\nType:      function\n\n\n\n\n\n747 Âµs Â± 58.1 Âµs per loop (mean Â± std. dev. of 7 runs, 50 loops each)\n\n\n\ndef matmul(a, b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            c[i,j] = torch.dot(a[i], b[:,j])\n    return c\n\n\ntest_close(t1, matmul(m1, m2))\n\n\n\n\n590 Âµs Â± 12 Âµs per loop (mean Â± std. dev. of 7 runs, 50 loops each)\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nm.shape\n\ntorch.Size([3, 3])\n\n\n\nc = tensor([10, 20, 30]);\n\n\nc.shape\n\ntorch.Size([3])\n\n\n\nt = c.expand_as(m)\n\n\nt.storage()\n\n 10\n 20\n 30\n[torch.storage._TypedStorage(dtype=torch.int64, device=cpu) of size 3]\n\n\n\nt.stride(), t.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\n\nc.unsqueeze(0).shape\n\ntorch.Size([1, 3])\n\n\n\nc.unsqueeze(1).shape\n\ntorch.Size([3, 1])\n\n\n\nc[None, :].shape\n\ntorch.Size([1, 3])\n\n\n\nc[None].shape\n\ntorch.Size([1, 3])\n\n\n\nc[:,None].shape\n\ntorch.Size([3, 1])\n\n\n\nc[...,None].shape\n\ntorch.Size([3, 1])\n\n\n\nt\n\ntensor([[10, 20, 30],\n        [10, 20, 30],\n        [10, 20, 30]])\n\n\n\nc\n\ntensor([10, 20, 30])\n\n\n\nc[None,:] * c[:,None]\n\ntensor([[100, 200, 300],\n        [200, 400, 600],\n        [300, 600, 900]])\n\n\n\ndigit = m1[0]\ndigit.shape, m2.shape\n\n(torch.Size([784]), torch.Size([784, 10]))\n\n\n\nm1.shape\n\ntorch.Size([5, 784])\n\n\n\ndigit[:,None].shape\n\ntorch.Size([784, 1])\n\n\n\nm2.shape\n\ntorch.Size([784, 10])\n\n\n\ndigit.unsqueeze(1).shape\n\ntorch.Size([784, 1])\n\n\n\ndef matmul(a, b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        c[i] = (a[i,:,None] * b).sum(dim=0)  # (ac, 1) * (br, bc)\n    return c\n\n\nm1[0, :].unsqueeze(-1).shape\n\ntorch.Size([784, 1])\n\n\n\ntest_close(t1, matmul(m1, m2))\n\n\n\n\n132 Âµs Â± 7.76 Âµs per loop (mean Â± std. dev. of 7 runs, 50 loops each)\n\n\n\ntr = matmul(x_train, weights)\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n\n\nCPU times: user 2.04 s, sys: 330 ms, total: 2.37 s\nWall time: 1.2 s\n\n\n\n\n\n1.2 s Â± 7.79 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n11.8 ms Â± 1.69 ms per loop (mean Â± std. dev. of 7 runs, 5 loops each)\n\n\n\n\nCUDA\n\nfrom numba import cuda\n\n\n# x_train.cuda()\n\n\nm1\n\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n\n\n\nm1.shape, m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nmr = torch.einsum('ik,kj-&gt;ikj', m1, m2)\nmr.shape\n\ntorch.Size([5, 784, 10])\n\n\n\nmr.sum(1)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ntorch.einsum('ik,kj-&gt;ij', m1, m2)\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ntest_close(mr.sum(1), torch.einsum('ik,kj-&gt;ij', m1, m2))"
  },
  {
    "objectID": "03_backprop.html",
    "href": "03_backprop.html",
    "title": "Start actual lesson",
    "section": "",
    "text": "import pdb\n# pdb.set_trace()\nimport torch\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.randn(3, 5).softmax(dim=1)\ntarget\n\ntensor([[0.0939, 0.0956, 0.2387, 0.3214, 0.2504],\n        [0.5152, 0.0805, 0.1735, 0.0819, 0.1489],\n        [0.0680, 0.0394, 0.0111, 0.2271, 0.6543]])\ninput\n\ntensor([[-1.4974,  0.6848,  0.0614, -1.3641, -0.1052],\n        [ 0.8279, -0.2838, -0.3835, -1.0058, -0.7883],\n        [-1.0865,  0.1394, -0.4530,  1.0402, -1.2003]], requires_grad=True)\nimport torch.nn as nn\nloss = nn.CrossEntropyLoss()\nloss??\nimport torch.nn.functional as F\nF.cross_entropy??\nimport pickle, gzip, math, os, time, shutil,torch,matplotlib as mpl, numpy as np\nfrom pathlib import Path\nfrom torch import tensor\nfrom fastcore.test import test_close\ntorch.manual_seed(42);\nimport matplotlib.pyplot as plt\n# TODO: experiment with different values here once we have images\nmpl.rcParams['image.cmap'] = 'gray'\nplt.plot(torch.range(0, 10), torch.range(0, 10));\n\n/var/folders/c8/mt_y_mg14_s14_slht8ds95w0000gn/T/ipykernel_22438/2324971393.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n  plt.plot(torch.range(0, 10), torch.range(0, 10));\ntorch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\nnp.set_printoptions(precision=2, linewidth=125)\npath_data = Path('data')\npath_gz = path_data/'mnist.pkl.gz'\nwith gzip.open(path_gz, 'rb') as f:\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n\n# import pdb; pdb.set_trace()\ntype(x_train), x_train.shape\n\n(numpy.ndarray, (50000, 784))\nx_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\ntype(x_train), x_train.shape\n\n(torch.Tensor, torch.Size([50000, 784]))\nfrom IPython.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;\"))"
  },
  {
    "objectID": "03_backprop.html#refactor-model",
    "href": "03_backprop.html#refactor-model",
    "title": "Start actual lesson",
    "section": "Refactor model",
    "text": "Refactor model\n\ndef f(a, b, *c):\n    print(a, b, *c, c)\n\n\nf(1, 2, 3, 4, 5)\n\n1 2 3 4 5 (3, 4, 5)\n\n\n\nclass Relu():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min_(0)\n        return self.out\n    \n    def backward(self):\n        self.inp.g = (self.inp &gt; 0).float() * self.out.g\n\n\nclass Lin():\n    def __init__(self, w, b):\n        self.w, self.b = w,b\n    \n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp @ self.w + self.b\n        return self.out\n    \n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\n\nclass Mse():\n    def __call__(self, inp, target):\n        self.inp,self.target = inp,target\n        self.out = mse(inp, target)\n        return self.out\n    \n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.target).unsqueeze(-1) / self.target.shape[0]\n\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n        self.loss = Mse()\n    \n    def __call__(self, x, target):\n        for l in self.layers: x = l(x)\n        return self.loss(x, target)\n    \n    def backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.0000000001)\n\n\ntest_close(x_train_g, x_train.g, eps=0.000000000001)\n\n\nprint(x_train_g[:5])\n\ntensor([[-0.01,  0.00,  0.00,  ...,  0.01,  0.01,  0.02],\n        [-0.01,  0.00,  0.00,  ...,  0.01,  0.01,  0.01],\n        [-0.00,  0.00,  0.00,  ...,  0.00, -0.00,  0.00],\n        [-0.00,  0.00,  0.00,  ..., -0.00,  0.00,  0.00],\n        [-0.01,  0.01,  0.00,  ..., -0.00,  0.00,  0.00]])\n\n\n\nprint(x_train.g[:5])\n\ntensor([[-0.01,  0.00,  0.00,  ...,  0.01,  0.01,  0.02],\n        [-0.01,  0.00,  0.00,  ...,  0.01,  0.01,  0.01],\n        [-0.00,  0.00,  0.00,  ...,  0.00, -0.00,  0.00],\n        [-0.00,  0.00,  0.00,  ..., -0.00,  0.00,  0.00],\n        [-0.01,  0.01,  0.00,  ..., -0.00,  0.00,  0.00]])\n\n\n\nAnother refactoring\n\nclass Module():\n    def __call__(self, *args, **kwargs):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def forward(self): raise Exception('not implemented')\n    def backward(self): self.bwd(self.out, *self.args)\n    def bwd(self): raise Exception('not implemented')\n\n\nclass Relu(Module):\n    def forward(self, inp):\n        return inp.clamp_min_(0)\n    \n    def bwd(self, out, inp):\n        inp.g = (inp &gt; 0).float() * out.g\n\n\na = Relu()\n\n\na(tensor(2))\n\ntensor(2)\n\n\n\na(tensor(-2))\n\ntensor(0)\n\n\n\nclass Lin(Module):\n    def __init__(self, w, b):\n        self.w, self.b = w,b\n    \n    def forward(self, inp):\n        return lin(inp, self.w, self.b)\n    \n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        self.w.g = inp.t() @ out.g\n        self.b.g = out.g.sum(0)\n\n\nclass Mse(Module):\n    def forward(self, inp, target):\n        return mse(inp, target)\n    \n    def bwd(self, out, inp, target):\n        inp.g = 2. * (inp.squeeze() - target).unsqueeze(-1) / target.shape[0]\n\n\nmodel = Model(w1, b1, w2, b2)\n\n\nloss = model(x_train, y_train)\n\n\nmodel.backward()\n\n\ntest_close(w2g, w2.g, eps=0.01)\ntest_close(b2g, b2.g, eps=0.01)\ntest_close(w1g, w1.g, eps=0.01)\ntest_close(b1g, b1.g, eps=0.0000000001)\ntest_close(x_train_g, x_train.g, eps=0.000000000001)\n\n\n\nAutograd\n\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Linear(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.w = torch.randn(n_in, n_out, requires_grad=True)\n        self.b = torch.randn(n_out, requires_grad=True)\n        \n    def forward(self, x):\n        return x@self.w + self.b\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [Linear(n_in, nh), nn.ReLU(), Linear(nh, n_out)]\n    \n    def __call__(self, inp, target):\n        for l in self.layers: inp = l(inp) # [B, n_out] dim\n        return F.mse_loss(inp, target.unsqueeze(-1))\n\n\nmodel = Model(m, nh, 1)\n\n\nloss = model(x_train, y_train)\nloss.backward()\n\n\nnn.Linear??\n\n\nl0 = model.layers[0]\nl0.b.grad.shape\n\ntorch.Size([50])\n\n\n\nx_train.shape\n\ntorch.Size([50000, 784])"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "Matplotlib_intro.html",
    "href": "Matplotlib_intro.html",
    "title": "fastai_course",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\nimport matplotlib as mpl\n\n\n\nfig, ax = plt.subplots()\nax.plot([1, 2, 3, 4], [1, 4, 2, 3])\n\n\n\n\n\nplt.subplots??\n\n\nfig, ax = plt.subplots()\nax.scatter([1, 2, 3, 4], [1, 4, 2, 3])\n\n&lt;matplotlib.collections.PathCollection at 0x13ae778b0&gt;\n\n\n\n\n\n\nfig1, axs1 = plt.subplot_mosaic([['left', 'right_top'], ['left', 'right_bottom']])\n\n\n\n\n\nnp.random.seed(1)\n\n\ndata = {\n    'a': np.arange(50),\n    'c': np.random.randint(0, 50, 50),\n    'd': np.random.randn(50)\n}\n\n\ndata['d'].max(), data['d'].min()\n\n(1.9999756374163042, -2.2254950845537786)\n\n\n\nnp.random.randn??\n\n\nplt.hist(np.random.randn(500));\n\n\n\n\n\nnp.arange(10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nnp.random.randint(0, 50, 50)\n\narray([23, 16,  9, 27, 30, 48, 49, 37, 21,  0, 31, 29, 18, 39, 44, 49, 44,\n       23, 36, 17, 25,  5, 43, 10, 21,  4, 43, 46, 45, 10, 12, 35, 17, 11,\n        4, 23, 26,  1, 16, 11, 37, 25, 24, 16, 15, 41, 20, 19, 45, 47])\n\n\n\nnp.random.randint??\n\n\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\n\nfig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\nplt.scatter('a', 'b', c='c', s='d', data=data);\n\n\n\n\n\nax.scatter??\n\n\ndata['d']\n\narray([8.91913759e+01, 1.73559653e+02, 6.31108331e+01, 9.07343356e+01,\n       3.74751973e+01, 4.73382754e+01, 4.54520824e+01, 8.53380627e+00,\n       1.50318838e+02, 1.16064112e+02, 4.82941397e+01, 1.80662901e+02,\n       9.15447608e+01, 6.97339847e+00, 8.49954772e+01, 5.06232307e+00,\n       7.05072381e+01, 1.67994195e+02, 1.99997564e+02, 1.15696327e+02,\n       1.93117963e+01, 1.55300741e+02, 1.00135380e+00, 6.49165533e+01,\n       1.55857293e+02, 2.38351806e+01, 1.08086617e+01, 1.01526292e+02,\n       9.59636776e+01, 1.27303148e+01, 6.98554164e+01, 2.22549508e+02,\n       1.26822935e+02, 1.39218201e+02, 1.92383191e+02, 9.91091266e+01,\n       3.80610804e+01, 7.47766359e+01, 1.31767750e+02, 1.33291008e+02,\n       6.84540250e+01, 1.25598233e+02, 4.96726354e+01, 3.29275803e+01,\n       1.18990846e-01, 3.07292625e+01, 1.83536999e+01, 5.52975151e+01,\n       1.55712804e+01, 5.72392274e+01])"
  }
]